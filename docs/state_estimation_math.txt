================================================================================
OBJECT GEOLOCALIZATION AND STATE ESTIMATION - MATHEMATICAL DOCUMENTATION
================================================================================

This document describes the mathematical transformations used in main_v2_state_est.py
for converting pixel coordinates to global LLA (Latitude, Longitude, Altitude).

================================================================================
1. COORDINATE FRAMES
================================================================================

1.1 Camera Frame (OpenCV Convention)
------------------------------------
- Origin: Camera optical center
- X-axis: Points RIGHT
- Y-axis: Points DOWN
- Z-axis: Points FORWARD (into the scene)

1.2 NED Frame (North-East-Down)
-------------------------------
- Origin: Reference point (Home position at Frame 0)
- X-axis: Points NORTH
- Y-axis: Points EAST
- Z-axis: Points DOWN

1.3 LLA (Geodetic Coordinates)
------------------------------
- Latitude: Degrees North (+) / South (-)
- Longitude: Degrees East (+) / West (-)
- Altitude: Meters above reference ellipsoid

================================================================================
2. COORDINATE RESCALING
================================================================================

Bounding boxes from object detection are at source resolution (e.g., 1920x1080).
Depth maps are at VGGT model resolution (e.g., 518x294).

Scale factors:
$$
s_x = \frac{W_{depth}}{W_{source}}, \quad s_y = \frac{H_{depth}}{H_{source}}
$$

Rescaled bounding box:
$$
\begin{aligned}
x_1' &= x_1 \cdot s_x \\
y_1' &= y_1 \cdot s_y \\
x_2' &= x_2 \cdot s_x \\
y_2' &= y_2 \cdot s_y
\end{aligned}
$$

Bounding box center in depth map coordinates:
$$
u = \frac{x_1' + x_2'}{2}, \quad v = \frac{y_1' + y_2'}{2}
$$

================================================================================
3. ROBUST DEPTH EXTRACTION
================================================================================

Given depth pixels $D$ within the rescaled bounding box:

1. Filter invalid depths:
$$
D_{valid} = \{d \in D : d > 0 \land d \neq \text{NaN}\}
$$

2. Background rejection (remove far pixels):
$$
\tau = P_{70}(D_{valid}) \quad \text{(70th percentile)}
$$
$$
D_{foreground} = \{d \in D_{valid} : d \leq \tau\}
$$

3. Representative depth:
$$
d_{obj} = \text{median}(D_{foreground})
$$

================================================================================
4. CAMERA INTRINSICS
================================================================================

Intrinsic matrix K (3x3):
$$
K = \begin{bmatrix}
f_x & 0 & c_x \\
0 & f_y & c_y \\
0 & 0 & 1
\end{bmatrix}
$$

Where:
- $f_x, f_y$: Focal lengths in pixels
- $c_x, c_y$: Principal point (image center for VGGT)

VGGT computes intrinsics from Field of View:
$$
f_y = \frac{H/2}{\tan(\text{FoV}_h / 2)}, \quad f_x = \frac{W/2}{\tan(\text{FoV}_w / 2)}
$$

================================================================================
5. PIXEL TO CAMERA FRAME (UNPROJECTION)
================================================================================

Given pixel coordinates $(u, v)$ and depth $d$, the 3D point in camera frame:

$$
\mathbf{P}_{cam} = \begin{bmatrix}
X_{cam} \\
Y_{cam} \\
Z_{cam}
\end{bmatrix}
= \begin{bmatrix}
(u - c_x) \cdot d / f_x \\
(v - c_y) \cdot d / f_y \\
d
\end{bmatrix}
$$

This is equivalent to:
$$
\mathbf{P}_{cam} = d \cdot K^{-1} \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}
$$

================================================================================
6. GIMBAL ROTATION MATRIX (CAMERA -> NED)
================================================================================

6.1 DJI Gimbal Angle Conventions
--------------------------------
- Pitch ($\theta$): Positive = looking UP, Negative = looking DOWN
- Yaw ($\psi$): Positive = rotate RIGHT (clockwise from above)
- Roll ($\phi$): Positive = right wing DOWN

6.2 Gimbal Mode Handling
------------------------

**Follow Yaw Mode (Default):**
- Pitch/Roll: Horizon-stabilized (absolute angles relative to horizon)
- Yaw: In DJI flight logs, GIMBAL.yaw is typically already absolute (North-referenced)
$$
\psi_{absolute} = \psi_{gimbal} + \delta_{magnetic} + \psi_{offset}
$$

Where:
- $\delta_{magnetic}$ = Magnetic declination (True North = Magnetic North + Declination)
- $\psi_{offset}$ = Additional calibration offset

**Alternative (if gimbal yaw is relative to drone):**
$$
\psi_{absolute} = \psi_{drone} + \psi_{gimbal} + \delta_{magnetic} + \psi_{offset}
$$

**FPV Mode:**
- Roll: Locked to drone body (not horizon-stabilized)
$$
\phi_{absolute} = \phi_{drone} + \phi_{gimbal}
$$
$$
\psi_{absolute} = \psi_{drone} + \psi_{gimbal} + \psi_{offset}
$$

**Free/Lock Mode:**
- All angles independent of drone heading
$$
\psi_{absolute} = \psi_{gimbal} + \psi_{offset}
$$

6.3 Rotation Matrix Construction
--------------------------------

Step 1: Camera to Body Frame (Neutral Gimbal)
When gimbal is at neutral position, camera axes map to body axes:
- Camera Z (forward) -> Body X (forward)
- Camera X (right) -> Body Y (right)
- Camera Y (down) -> Body Z (down)

$$
R_{cam \to body}^{neutral} = \begin{bmatrix}
0 & 0 & 1 \\
1 & 0 & 0 \\
0 & 1 & 0
\end{bmatrix}
$$

Step 2: Gimbal Rotation (ZYX Euler - Yaw, Pitch, Roll)

Individual rotation matrices:
$$
R_z(\psi) = \begin{bmatrix}
\cos\psi & -\sin\psi & 0 \\
\sin\psi & \cos\psi & 0 \\
0 & 0 & 1
\end{bmatrix}
$$

$$
R_y(\theta) = \begin{bmatrix}
\cos\theta & 0 & \sin\theta \\
0 & 1 & 0 \\
-\sin\theta & 0 & \cos\theta
\end{bmatrix}
$$

$$
R_x(\phi) = \begin{bmatrix}
1 & 0 & 0 \\
0 & \cos\phi & -\sin\phi \\
0 & \sin\phi & \cos\phi
\end{bmatrix}
$$

Combined gimbal rotation (intrinsic ZYX):
$$
R_{gimbal} = R_z(\psi_{absolute}) \cdot R_y(\theta) \cdot R_x(\phi)
$$

Step 3: Full Transformation
$$
R_{cam \to NED} = R_{gimbal} \cdot R_{cam \to body}^{neutral}
$$

================================================================================
7. CAMERA TO NED TRANSFORMATION
================================================================================

Point in NED frame (relative to UAV):
$$
\mathbf{P}_{NED}^{rel} = R_{cam \to NED} \cdot \mathbf{P}_{cam}
$$

Absolute point in NED frame:
$$
\mathbf{P}_{NED}^{abs} = \mathbf{P}_{NED}^{rel} + \mathbf{T}_{UAV}^{NED}
$$

Where $\mathbf{T}_{UAV}^{NED}$ is the UAV position in NED frame relative to home.

================================================================================
8. LLA TO NED CONVERSION
================================================================================

UAV position from LLA to NED (relative to home):

Using WGS84 ellipsoid approximations:
$$
N = (\text{lat} - \text{lat}_0) \cdot R_N
$$
$$
E = (\text{lon} - \text{lon}_0) \cdot R_E \cdot \cos(\text{lat}_0)
$$
$$
D = \text{alt}_0 - \text{alt}
$$

Where:
- $R_N \approx 111320$ m/degree (meridional radius)
- $R_E \approx 111320$ m/degree (prime vertical radius at equator)

More accurate: Use pymap3d.geodetic2ned() which accounts for ellipsoid curvature.

================================================================================
9. NED TO LLA CONVERSION
================================================================================

Converting object NED position back to LLA:

$$
\text{lat} = \text{lat}_0 + \frac{N}{R_N}
$$
$$
\text{lon} = \text{lon}_0 + \frac{E}{R_E \cdot \cos(\text{lat}_0)}
$$
$$
\text{alt} = \text{alt}_0 - D
$$

More accurate: Use pymap3d.ned2geodetic().

================================================================================
10. VELOCITY ESTIMATION
================================================================================

Position time series: $\mathbf{P}_i^{NED}$ at frame $i$

Time step:
$$
\Delta t = \frac{1}{\text{FPS}}
$$

Central difference (interior points):
$$
\mathbf{V}_i = \frac{\mathbf{P}_{i+1}^{NED} - \mathbf{P}_{i-1}^{NED}}{2 \Delta t}
$$

Forward difference (first point):
$$
\mathbf{V}_0 = \frac{\mathbf{P}_1^{NED} - \mathbf{P}_0^{NED}}{\Delta t}
$$

Backward difference (last point):
$$
\mathbf{V}_n = \frac{\mathbf{P}_n^{NED} - \mathbf{P}_{n-1}^{NED}}{\Delta t}
$$

Moving average smoothing (window size $w$):
$$
\mathbf{V}_i^{smooth} = \frac{1}{|W_i|} \sum_{j \in W_i} \mathbf{V}_j
$$

Where $W_i = \{j : |j - i| \leq w/2\}$

Horizontal speed:
$$
\text{speed} = \sqrt{V_N^2 + V_E^2}
$$

================================================================================
11. CAMERA HEADING PROJECTION (NE PLANE)
================================================================================

Camera look direction in camera frame:
$$
\mathbf{d}_{cam} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}
$$

Transform to NED:
$$
\mathbf{d}_{NED} = R_{cam \to NED} \cdot \mathbf{d}_{cam}
$$

Project onto NE plane (ignore Down component):
$$
\mathbf{d}_{NE} = \begin{bmatrix} d_N \\ d_E \end{bmatrix}
$$

Normalize to unit vector:
$$
\hat{\mathbf{d}}_{NE} = \frac{\mathbf{d}_{NE}}{|\mathbf{d}_{NE}|}
$$

================================================================================
12. WEB MERCATOR PROJECTION (FOR VISUALIZATION)
================================================================================

LLA to Web Mercator (EPSG:3857):
$$
x = \text{lon} \cdot \frac{20037508.34}{180}
$$
$$
y = \ln\left(\tan\left(\frac{90 + \text{lat}}{360} \cdot \pi\right)\right) \cdot \frac{180}{\pi} \cdot \frac{20037508.34}{180}
$$

================================================================================
13. SUMMARY OF FULL PIPELINE
================================================================================

Input: Pixel $(u, v)$ in source resolution, depth map, telemetry

1. Rescale pixel to depth map coordinates
2. Extract robust foreground depth $d$
3. Get camera intrinsics $K$ for current frame
4. Unproject to camera frame: $\mathbf{P}_{cam}$
5. Build rotation matrix $R_{cam \to NED}$ from gimbal angles and mode
6. Convert UAV LLA to NED: $\mathbf{T}_{UAV}$
7. Transform object to NED: $\mathbf{P}_{NED} = R \cdot \mathbf{P}_{cam} + \mathbf{T}_{UAV}$
8. Convert NED to LLA: $(\text{lat}, \text{lon}, \text{alt})$
9. Estimate velocity from position history

Output: Object LLA position, depth, velocity vector, speed

================================================================================
END OF DOCUMENT
================================================================================
